{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEAkC59iln4E",
        "outputId": "2c130fa2-e75e-4f5e-b99c-e1f3a3872fb1"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n",
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1izqFgClITk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from conlleval import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAPnFGAVlMdR"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpLPxkdQlVj4"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZdp1xHClwW8"
      },
      "outputs": [],
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u56rdOdtl38f"
      },
      "outputs": [],
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jx1T5T2mdCd"
      },
      "outputs": [],
      "source": [
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            if len(tokens) > 0:\n",
        "                f.write(\n",
        "                    str(len(tokens))\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(tokens)\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(map(str, ner_tags))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "\n",
        "# os.mkdir(\"data\")\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH3skDIfmevZ",
        "outputId": "ca243b1e-b513-47dc-c350-d5f74d66849f"
      },
      "outputs": [],
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o-PG7nkmhii",
        "outputId": "1937a150-12e2-47cd-cc72-dda64114ae62"
      },
      "outputs": [],
      "source": [
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etun6UhQmjPI"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSjUgoHemlAx",
        "outputId": "7350fcd1-4458-4562-dcf3-8c2ec1faa10a"
      },
      "outputs": [],
      "source": [
        "print(list(train_data.take(1).as_numpy_iterator()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1rv3gj4mmM0"
      },
      "outputs": [],
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3CZFRqCmpY9"
      },
      "outputs": [],
      "source": [
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False, reduction='none'\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPx4PcGpn1Sd",
        "outputId": "5b7ba685-662a-4939-8ca3-b461942f3291"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7weaMUaFmrYq",
        "outputId": "f9238d48-fd55-4485-c945-f4edc126aad0"
      },
      "outputs": [],
      "source": [
        "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
        "ner_model.fit(train_dataset, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHayLR77mu9Z",
        "outputId": "38c671f1-7dd8-4fd6-f73a-997d923feb1f"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(dataset):\n",
        "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
        "\n",
        "    for x, y in dataset:\n",
        "        output = ner_model.predict(x, verbose=0)\n",
        "        predictions = np.argmax(output, axis=-1)\n",
        "        predictions = np.reshape(predictions, [-1])\n",
        "\n",
        "        true_tag_ids = np.reshape(y, [-1])\n",
        "\n",
        "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
        "        true_tag_ids = true_tag_ids[mask]\n",
        "        predicted_tag_ids = predictions[mask]\n",
        "\n",
        "        all_true_tag_ids.append(true_tag_ids)\n",
        "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
        "\n",
        "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
        "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
        "\n",
        "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
        "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
        "\n",
        "    evaluate(real_tags, predicted_tags)\n",
        "\n",
        "\n",
        "calculate_metrics(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-gu2Fdpnnqf"
      },
      "outputs": [],
      "source": [
        "ner_model.save(\"named_entity_recognizer.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k404L9h-yoVs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
